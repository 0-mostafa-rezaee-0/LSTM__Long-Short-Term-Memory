{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">LSTM Time Series Forecasting Training Notebook</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates the process of training various LSTM models for time series forecasting using the components from the LSTM_Dockerized project. It covers data preparation, model creation, training, and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Long Short-Term Memory (LSTM) networks are a type of recurrent neural network (RNN) architecture designed to model temporal sequences and their long-range dependencies more accurately than conventional RNNs. This notebook walks through the complete workflow for training LSTM models for time series forecasting, using the components from the LSTM_Dockerized project.\n",
    "\n",
    "We'll cover:\n",
    "- Data loading and preprocessing\n",
    "- Creating sequences for LSTM input\n",
    "- Building and configuring different LSTM architectures\n",
    "- Training and evaluating the models\n",
    "- Visualizing results and model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import json\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Configure plot style\n",
    "plt.style.use('ggplot')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (15, 6)\n",
    "\n",
    "# Make TensorFlow less verbose\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "\n",
    "# Check TensorFlow version\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "# Check if GPU is available\n",
    "print(f\"GPU Available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
    "print(f\"GPU Devices: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import custom data preprocessing utilities\n",
    "from utils.preprocessing import normalize_data, create_sequences, train_val_test_split, detect_stationarity\n",
    "\n",
    "# Load sample time series data (e.g., stock prices)\n",
    "# For this example, let's use a synthetic dataset. In practice, you would load your actual data.\n",
    "def generate_sample_data(n_samples=1000):\n",
    "    \"\"\"Generate a synthetic time series with trend, seasonality, and noise.\"\"\"\n",
    "    time = np.arange(n_samples)\n",
    "    # Trend component\n",
    "    trend = 0.001 * time\n",
    "    # Seasonal component (multiple seasonal patterns)\n",
    "    season1 = 0.5 * np.sin(2 * np.pi * time / 50)  # 50-day cycle\n",
    "    season2 = 0.2 * np.sin(2 * np.pi * time / 7)   # Weekly cycle \n",
    "    # Random noise\n",
    "    noise = 0.1 * np.random.randn(n_samples)\n",
    "    # Combine components\n",
    "    data = trend + season1 + season2 + noise\n",
    "    return pd.DataFrame({'value': data})\n",
    "\n",
    "# Generate and plot sample data\n",
    "data = generate_sample_data(1000)\n",
    "data.index = pd.date_range(start='2020-01-01', periods=len(data), freq='D')\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "print(data.head())\n",
    "\n",
    "# Plot the time series\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(data.index, data['value'])\n",
    "plt.title('Sample Time Series Data')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check for stationarity\n",
    "is_stationary, pvalue, test_statistic = detect_stationarity(data['value'], test='adfuller')\n",
    "print(f\"Is stationary: {is_stationary}\")\n",
    "print(f\"p-value: {pvalue:.4f}\")\n",
    "print(f\"Test statistic: {test_statistic:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "normalized_data, scaler = normalize_data(data, method='minmax')\n",
    "print(\"Normalized data head:\")\n",
    "print(normalized_data.head())\n",
    "\n",
    "# Create sequences for LSTM input\n",
    "seq_length = 30  # Look back 30 time steps\n",
    "horizon = 1      # Predict 1 step ahead\n",
    "\n",
    "X, y = create_sequences(normalized_data, seq_length=seq_length, horizon=horizon)\n",
    "print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(\n",
    "    X, y, val_size=0.2, test_size=0.1, shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}, {y_val.shape}\")\n",
    "print(f\"Test set: {X_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import model creation utilities\n",
    "from models.model_factory import create_model, create_model_from_config\n",
    "from models.training.trainer import train_model\n",
    "from models.evaluation.evaluator import evaluate_model\n",
    "from models.training.callbacks import create_callbacks\n",
    "\n",
    "# Define a model configuration\n",
    "model_config = {\n",
    "    \"architecture\": \"vanilla_lstm\",\n",
    "    \"input_shape\": (seq_length, 1),\n",
    "    \"lstm_units\": 50,\n",
    "    \"dropout_rate\": 0.2,\n",
    "    \"output_units\": 1,\n",
    "    \"activation\": \"linear\",\n",
    "    \"compile\": {\n",
    "        \"optimizer\": \"adam\",\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"loss\": \"mse\",\n",
    "        \"metrics\": [\"mae\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create the model\n",
    "model = create_model_from_config(model_config)\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "patience = 15\n",
    "\n",
    "# Create directory for model checkpoints\n",
    "checkpoint_dir = os.path.join(os.getcwd(), 'models', 'checkpoints')\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "checkpoint_path = os.path.join(checkpoint_dir, 'best_model.h5')\n",
    "\n",
    "# Train the model\n",
    "history = train_model(\n",
    "    model=model,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val, \n",
    "    y_val=y_val,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    early_stopping=True,\n",
    "    patience=patience,\n",
    "    checkpoint_path=checkpoint_path,\n",
    "    tensorboard=True,\n",
    "    log_dir='logs',\n",
    "    save_best_model=True\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['mae'], label='Training MAE')\n",
    "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Absolute Error (MAE)')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation MAE')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the model on test data\n",
    "from utils.metrics import calculate_all_metrics\n",
    "\n",
    "# Generate predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = evaluate_model(model, X_test, y_test, scaler)\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"Evaluation Metrics on Test Data:\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Plot predictions vs actual values (using inverse transformation for actual scale)\n",
    "# First, reshape the data for inverse transformation\n",
    "y_test_reshaped = y_test.reshape(-1, 1)\n",
    "y_pred_reshaped = y_pred.reshape(-1, 1)\n",
    "\n",
    "# Inverse transform to get back to original scale\n",
    "y_test_inv = scaler.inverse_transform(y_test_reshaped)\n",
    "y_pred_inv = scaler.inverse_transform(y_pred_reshaped)\n",
    "\n",
    "# Plot the predictions\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(y_test_inv, label='Actual Values', color='blue')\n",
    "plt.plot(y_pred_inv, label='Predictions', color='red', linestyle='--')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Model Predictions vs Actual Values on Test Data')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Trying Different LSTM Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Function to train and evaluate a model with a specific configuration\n",
    "def train_and_evaluate_model(architecture_name, hyperparams=None):\n",
    "    \"\"\"Train and evaluate a model with a specific architecture.\"\"\"\n",
    "    # Default hyperparameters if not provided\n",
    "    if hyperparams is None:\n",
    "        hyperparams = {}\n",
    "    \n",
    "    # Base configuration\n",
    "    config = {\n",
    "        \"architecture\": architecture_name,\n",
    "        \"input_shape\": (seq_length, 1),\n",
    "        \"output_units\": 1,\n",
    "        \"activation\": \"linear\",\n",
    "        \"compile\": {\n",
    "            \"optimizer\": \"adam\",\n",
    "            \"learning_rate\": 0.001,\n",
    "            \"loss\": \"mse\",\n",
    "            \"metrics\": [\"mae\"]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Update with provided hyperparameters\n",
    "    for key, value in hyperparams.items():\n",
    "        if key not in [\"architecture\", \"compile\"]:\n",
    "            config[key] = value\n",
    "        elif key == \"compile\":\n",
    "            for compile_key, compile_value in value.items():\n",
    "                config[\"compile\"][compile_key] = compile_value\n",
    "    \n",
    "    # Create and compile the model\n",
    "    print(f\"Training {architecture_name} model...\")\n",
    "    model = create_model_from_config(config)\n",
    "    \n",
    "    # Train the model\n",
    "    history = train_model(\n",
    "        model=model,\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        X_val=X_val, \n",
    "        y_val=y_val,\n",
    "        epochs=50,  # Reduced epochs for demonstration\n",
    "        batch_size=batch_size,\n",
    "        early_stopping=True,\n",
    "        patience=10,\n",
    "        save_best_model=False,\n",
    "        verbose=0  # Less verbose output\n",
    "    )\n",
    "    \n",
    "    # Evaluate the model\n",
    "    metrics = evaluate_model(model, X_test, y_test, scaler)\n",
    "    \n",
    "    return model, history, metrics\n",
    "\n",
    "# Define architectures to try\n",
    "architectures = [\n",
    "    {\n",
    "        \"name\": \"vanilla_lstm\",\n",
    "        \"hyperparams\": {\"lstm_units\": 50, \"dropout_rate\": 0.2}\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"stacked_lstm\",\n",
    "        \"hyperparams\": {\"lstm_units\": [50, 25], \"dropout_rate\": 0.2}\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"bidirectional_lstm\",\n",
    "        \"hyperparams\": {\"lstm_units\": 50, \"dropout_rate\": 0.2}\n",
    "    }\n",
    "]\n",
    "\n",
    "# Train and evaluate each architecture\n",
    "results = {}\n",
    "\n",
    "for arch in architectures:\n",
    "    print(f\"\\n{'-'*50}\")\n",
    "    print(f\"Training {arch['name']} architecture\")\n",
    "    print(f\"{'-'*50}\")\n",
    "    \n",
    "    model, history, metrics = train_and_evaluate_model(\n",
    "        architecture_name=arch[\"name\"],\n",
    "        hyperparams=arch[\"hyperparams\"]\n",
    "    )\n",
    "    \n",
    "    results[arch[\"name\"]] = {\n",
    "        \"model\": model,\n",
    "        \"history\": history,\n",
    "        \"metrics\": metrics\n",
    "    }\n",
    "    \n",
    "    print(f\"Test metrics for {arch['name']}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Compare the performance of different architectures\n",
    "from models.evaluation.model_comparison import compare_models\n",
    "\n",
    "# Extract the metrics from the results\n",
    "metrics_comparison = {}\n",
    "for name, result in results.items():\n",
    "    metrics_comparison[name] = result[\"metrics\"]\n",
    "\n",
    "# Compare RMSE and MAE across models\n",
    "metrics_to_compare = ['rmse', 'mae', 'mape']\n",
    "model_names = list(metrics_comparison.keys())\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "for i, metric in enumerate(metrics_to_compare):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    values = [metrics_comparison[name][metric] for name in model_names]\n",
    "    plt.bar(model_names, values)\n",
    "    plt.title(f'{metric.upper()} Comparison')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylabel(metric.upper())\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a comparison table\n",
    "comparison_df = pd.DataFrame(\n",
    "    {name: {metric: metrics_comparison[name][metric] for metric in metrics_to_compare} \n",
    "     for name in model_names}\n",
    ").T\n",
    "\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameter tuning with grid search\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "def tune_hyperparameters(architecture, param_grid):\n",
    "    \"\"\"Tune hyperparameters using grid search.\"\"\"\n",
    "    best_val_loss = float('inf')\n",
    "    best_params = None\n",
    "    all_results = []\n",
    "    \n",
    "    # Iterate through parameter combinations\n",
    "    for params in ParameterGrid(param_grid):\n",
    "        print(f\"Trying parameters: {params}\")\n",
    "        \n",
    "        # Create model config\n",
    "        config = {\n",
    "            \"architecture\": architecture,\n",
    "            \"input_shape\": (seq_length, 1),\n",
    "            \"output_units\": 1,\n",
    "            \"activation\": \"linear\",\n",
    "            **params,\n",
    "            \"compile\": {\n",
    "                \"optimizer\": \"adam\",\n",
    "                \"learning_rate\": params.pop(\"learning_rate\", 0.001),\n",
    "                \"loss\": \"mse\",\n",
    "                \"metrics\": [\"mae\"]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Create model\n",
    "        model = create_model_from_config(config)\n",
    "        \n",
    "        # Train model\n",
    "        history = train_model(\n",
    "            model=model,\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            X_val=X_val, \n",
    "            y_val=y_val,\n",
    "            epochs=30,  # Reduced for faster tuning\n",
    "            batch_size=batch_size,\n",
    "            early_stopping=True,\n",
    "            patience=5,\n",
    "            save_best_model=False,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Get the best validation loss\n",
    "        val_loss = min(history.history['val_loss'])\n",
    "        \n",
    "        # Evaluate on test data\n",
    "        test_metrics = evaluate_model(model, X_test, y_test, scaler)\n",
    "        \n",
    "        # Save results\n",
    "        result = {\n",
    "            \"params\": params,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"test_metrics\": test_metrics\n",
    "        }\n",
    "        all_results.append(result)\n",
    "        \n",
    "        # Check if this is the best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_params = params\n",
    "            print(f\"New best model found! Validation loss: {val_loss:.4f}\")\n",
    "    \n",
    "    return best_params, all_results\n",
    "\n",
    "# Define parameter grid for vanilla LSTM\n",
    "param_grid = {\n",
    "    \"lstm_units\": [32, 64, 128],\n",
    "    \"dropout_rate\": [0.1, 0.2, 0.3],\n",
    "    \"learning_rate\": [0.01, 0.001]\n",
    "}\n",
    "\n",
    "# Run hyperparameter tuning\n",
    "best_params, all_results = tune_hyperparameters(\"vanilla_lstm\", param_grid)\n",
    "\n",
    "print(\"\\nHyperparameter Tuning Results:\")\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best validation loss: {min(result['val_loss'] for result in all_results):.4f}\")\n",
    "\n",
    "# Plot the validation loss for different parameter combinations\n",
    "plt.figure(figsize=(15, 6))\n",
    "results_df = pd.DataFrame([\n",
    "    {**r[\"params\"], \"val_loss\": r[\"val_loss\"]} \n",
    "    for r in all_results\n",
    "])\n",
    "\n",
    "# Sort by validation loss\n",
    "results_df = results_df.sort_values(\"val_loss\")\n",
    "print(\"\\nTop 5 parameter combinations:\")\n",
    "print(results_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Training the Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Train the best model with the tuned hyperparameters\n",
    "best_config = {\n",
    "    \"architecture\": \"vanilla_lstm\",\n",
    "    \"input_shape\": (seq_length, 1),\n",
    "    \"output_units\": 1,\n",
    "    \"activation\": \"linear\",\n",
    "    **best_params,\n",
    "    \"compile\": {\n",
    "        \"optimizer\": \"adam\",\n",
    "        \"learning_rate\": best_params.get(\"learning_rate\", 0.001),\n",
    "        \"loss\": \"mse\",\n",
    "        \"metrics\": [\"mae\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create the best model\n",
    "best_model = create_model_from_config(best_config)\n",
    "\n",
    "# Create directory for the best model\n",
    "best_model_dir = os.path.join(os.getcwd(), 'models', 'best_model')\n",
    "os.makedirs(best_model_dir, exist_ok=True)\n",
    "best_model_path = os.path.join(best_model_dir, 'tuned_model.h5')\n",
    "\n",
    "# Train with more epochs for the final model\n",
    "history = train_model(\n",
    "    model=best_model,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val, \n",
    "    y_val=y_val,\n",
    "    epochs=100,\n",
    "    batch_size=batch_size,\n",
    "    early_stopping=True,\n",
    "    patience=15,\n",
    "    checkpoint_path=best_model_path,\n",
    "    save_best_model=True\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.legend()\n",
    "plt.title('Best Model: Training and Validation Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['mae'], label='Training MAE')\n",
    "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Absolute Error (MAE)')\n",
    "plt.legend()\n",
    "plt.title('Best Model: Training and Validation MAE')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the best model\n",
    "best_metrics = evaluate_model(best_model, X_test, y_test, scaler)\n",
    "\n",
    "print(\"Best Model Test Metrics:\")\n",
    "for metric, value in best_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Making Forecasts with the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Generate forecasts with the best model\n",
    "def generate_forecast(model, initial_sequence, steps=30, scaler=None):\n",
    "    \"\"\"Generate a multi-step forecast using the trained model.\"\"\"\n",
    "    forecast = []\n",
    "    current_sequence = initial_sequence.copy()\n",
    "    \n",
    "    # Make predictions step by step\n",
    "    for _ in range(steps):\n",
    "        # Predict the next value\n",
    "        pred = model.predict(current_sequence.reshape(1, *current_sequence.shape))\n",
    "        forecast.append(pred[0, 0])\n",
    "        \n",
    "        # Update the sequence for the next prediction (remove oldest, add newest)\n",
    "        current_sequence = np.append(current_sequence[1:], pred[0, 0])\n",
    "        current_sequence = current_sequence.reshape(initial_sequence.shape)\n",
    "    \n",
    "    # Convert predictions back to the original scale if a scaler is provided\n",
    "    if scaler is not None:\n",
    "        forecast = scaler.inverse_transform(np.array(forecast).reshape(-1, 1)).flatten()\n",
    "    \n",
    "    return forecast\n",
    "\n",
    "# Use the last sequence from the test set as the starting point\n",
    "initial_sequence = X_test[-1]\n",
    "\n",
    "# Generate a 30-day forecast\n",
    "forecast_steps = 30\n",
    "forecast = generate_forecast(best_model, initial_sequence, steps=forecast_steps, scaler=scaler)\n",
    "\n",
    "# Create dates for the forecast\n",
    "last_date = data.index[-1]\n",
    "forecast_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=forecast_steps, freq='D')\n",
    "\n",
    "# Plot the historical data and the forecast\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Historical data\n",
    "plt.plot(data.index[-100:], data['value'].values[-100:], label='Historical Data')\n",
    "\n",
    "# Forecast\n",
    "plt.plot(forecast_dates, forecast, label='Forecast', color='red', linestyle='--')\n",
    "\n",
    "# Add a vertical line at the forecast start\n",
    "plt.axvline(x=last_date, color='black', linestyle='-', alpha=0.2)\n",
    "plt.fill_between(forecast_dates, forecast, alpha=0.2, color='red')\n",
    "\n",
    "plt.title('Time Series Forecast')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Saving and Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "model_save_path = os.path.join(os.getcwd(), 'models', 'final_model.h5')\n",
    "best_model.save(model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "# Save the scaler for future use\n",
    "import pickle\n",
    "scaler_path = os.path.join(os.getcwd(), 'models', 'scaler.pkl')\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(f\"Scaler saved to {scaler_path}\")\n",
    "\n",
    "# Save the model configuration\n",
    "config_path = os.path.join(os.getcwd(), 'models', 'model_config.json')\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(best_config, f, indent=4)\n",
    "print(f\"Model configuration saved to {config_path}\")\n",
    "\n",
    "# Example of how to load the model and scaler\n",
    "def load_model_and_scaler(model_path, scaler_path):\n",
    "    \"\"\"Load a saved model and scaler.\"\"\"\n",
    "    # Load the model\n",
    "    from tensorflow.keras.models import load_model\n",
    "    model = load_model(model_path)\n",
    "    \n",
    "    # Load the scaler\n",
    "    with open(scaler_path, 'rb') as f:\n",
    "        scaler = pickle.load(f)\n",
    "    \n",
    "    return model, scaler\n",
    "\n",
    "# The loaded model and scaler could be used for new predictions\n",
    "# loaded_model, loaded_scaler = load_model_and_scaler(model_save_path, scaler_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we've demonstrated the complete workflow for training LSTM models for time series forecasting:\n",
    "\n",
    "1. We loaded and preprocessed time series data, creating sequences suitable for LSTM input.\n",
    "2. We built and trained various LSTM architectures, from simple Vanilla LSTM to more complex Stacked and Bidirectional LSTMs.\n",
    "3. We evaluated model performance using multiple metrics and compared different architectures.\n",
    "4. We performed hyperparameter tuning to find the optimal configuration.\n",
    "5. We trained a final model with the best parameters and used it to generate forecasts.\n",
    "6. We saved the model, scaler, and configuration for future use.\n",
    "\n",
    "This workflow can be adapted for different time series forecasting tasks by adjusting the data preprocessing, model architecture, and training parameters to suit the specific requirements of your application.\n",
    "\n",
    "Next steps could include:\n",
    "- Trying different feature engineering approaches\n",
    "- Incorporating exogenous variables into the model\n",
    "- Experimenting with more advanced architectures like Seq2Seq models or attention mechanisms\n",
    "- Deploying the model for real-time forecasting"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
